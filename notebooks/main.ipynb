{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_spark, median_age_class, north_south_latitude, west_east_longitude\n",
    "from pyspark.sql.functions import col, stddev, min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório dos dados de entrada, não inclusos no git.\n",
    "relative_ca_h_t_dir = '../raw_data/california_housing_train.csv'\n",
    "\n",
    "# Diretório para salvar dados processados, não inclusos no git.\n",
    "target_processed_data_dir = '../processed_data'\n",
    "processed_ca_housing_dir = f'{target_processed_data_dir}/ca_housing/ca_housing.parquet'\n",
    "\n",
    "views_dir = '../views'\n",
    "ca_housing_query_dir = f'{views_dir}/ca_analysis.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialização do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ca_housing = raw_ca_housing = spark.read.options(header = True, delimiter=',').csv(relative_ca_h_t_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ca_housing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Exploração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 - Coluna com maior desvio padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_devs = raw_ca_housing.select([stddev(column) for column in raw_ca_housing.columns]).collect()\n",
    "std_devs = [column for column in std_devs[0]]\n",
    "\n",
    "greatest_std_dev_value = max(std_devs)\n",
    "greatest_std_dev_column = raw_ca_housing.columns[std_devs.index(greatest_std_dev_value)]\n",
    "\n",
    "print(f'Greatest standard deviation column: {greatest_std_dev_column}: {greatest_std_dev_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Valor mínimo e máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_agg = raw_ca_housing.select(spark_min(greatest_std_dev_column), spark_max(greatest_std_dev_column)).collect()\n",
    "[greatest_std_dev_max, greatest_std_dev_min] = [i for i in column_agg[0]]\n",
    "\n",
    "print(f'{greatest_std_dev_column}\\nMin value: {greatest_std_dev_min}\\nMax value: {greatest_std_dev_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Trabalhando com colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 -  Criar coluna hma_cat, baseada na coluna housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ca_housing = processed_ca_housing.withColumn('hma_cat', median_age_class(col('housing_median_age').cast(FloatType())))\n",
    "processed_ca_housing.select('housing_median_age', 'hma_cat').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Criar coluna c_ns (e c_ol)\n",
    "PS: A regra para longitude foi alterada para retornar `oeste` ou `leste`, uma vez que é latitude que determina a região relativa de norte e sul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ca_housing = processed_ca_housing.withColumn('c_ns', north_south_latitude(col('latitude').cast(FloatType())))\n",
    "\n",
    "processed_ca_housing = processed_ca_housing.withColumn('c_ol', west_east_longitude(col('longitude').cast(FloatType())))\n",
    "processed_ca_housing.select('longitude', 'c_ol', 'latitude', 'c_ns').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Renomar as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ca_housing = processed_ca_housing\\\n",
    "    .withColumnRenamed('hma_cat', 'age')\\\n",
    "    .withColumnRenamed('c_ns', 'california_ns_region')\\\n",
    "    .withColumnRenamed('c_ol', 'california_ol_region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gravando o resultado em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ca_housing.select('age',\n",
    "    'california_ns_region',\n",
    "    'california_ol_region',\n",
    "    'total_rooms',\n",
    "    'total_bedrooms',\n",
    "    'population',\n",
    "    'households',\n",
    "    'median_house_value')\\\n",
    "    .write.format('parquet').mode('overwrite').save(processed_ca_housing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Agregações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_housing = spark.read.parquet(processed_ca_housing_dir)\n",
    "\n",
    "ca_housing.createOrReplaceTempView('ca_housing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Criação da análise: Soma de população e média dos valores médios das casas agrupados por idade e região.\n",
    "PS: O desafio pede ordenação decrescente por `median_house_value`, porém, por se tratar de uma query com agregações, não há como incluir colunas que não servem de agrupamento ou agregação. Nesse caso, foi interpretado que a ordenação descrescente é por `m_median_house_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_housing_query = open(ca_housing_query_dir).read()\n",
    "\n",
    "try:\n",
    "    ca_housing_view = spark.sql(ca_housing_query)\n",
    "    ca_housing_view.show(10, truncate = False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bf0cc4c0169943300cc1e2a7f17c29feeb6fa1ad46cb9244f1a1309c84b3603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
